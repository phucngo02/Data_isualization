{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"5tz3InvVoYIs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669883686304,"user_tz":-420,"elapsed":18367,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}},"outputId":"ed2c8f60-ca8b-462c-d1dd-45b2cd0781d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["pip install -r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt"],"metadata":{"id":"AWPhBlI6S8ub","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669883693652,"user_tz":-420,"elapsed":7352,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}},"outputId":"582cf73f-4d11-4b17-9ffb-79dd10a26664"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tslearn\n","  Downloading tslearn-0.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (875 kB)\n","\u001b[K     |████████████████████████████████| 875 kB 6.5 MB/s \n","\u001b[?25hCollecting warmup-scheduler\n","  Downloading warmup_scheduler-0.3.tar.gz (2.1 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (1.2.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (1.0.2)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (from tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (0.29.32)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (1.7.3)\n","Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (0.56.4)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba->tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (57.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (4.13.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (3.10.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->tslearn->-r /content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/requirements.txt (line 1)) (3.1.0)\n","Building wheels for collected packages: warmup-scheduler\n","  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3-py3-none-any.whl size=2983 sha256=7aed59d409e485e6577a043b4c284c6c9cb96de01a6daa9362a2025d7cf5d622\n","  Stored in directory: /root/.cache/pip/wheels/f2/ce/4a/215c4f0add432420ff90fe04656bf2664ddfac7302e2b6fe51\n","Successfully built warmup-scheduler\n","Installing collected packages: warmup-scheduler, tslearn\n","Successfully installed tslearn-0.5.2 warmup-scheduler-0.3\n"]}]},{"cell_type":"code","source":["import sys\n","#sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')\n","sys.path.append('/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet')"],"metadata":{"id":"Da6YXFf8TIUu","executionInfo":{"status":"ok","timestamp":1669883698400,"user_tz":-420,"elapsed":2,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Thư viện cần thiết"],"metadata":{"id":"5vJ5X1FqTL_G"}},{"cell_type":"code","source":["from tslearn.metrics import dtw, dtw_path\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from warmup_scheduler import GradualWarmupScheduler\n","from utils.cyclic_scheduler import CyclicLRWithRestarts\n","from utils.dilate_loss import dilate_loss\n","from utils.adamw import AdamW\n","from utils.metrics import RMSLE\n","from utils.support import *\n","from utils.prepare_QLD import test_qld_single_station\n","from utils.prepare_QLD_Level import test_qld_single_station_Level\n","from utils.early_stopping import EarlyStopping\n","from models.DualHead_NoShare import Shared_Encoder, Cross_Attention, Decoder, DualSSIM\n","import pandas as pd\n","import torch, random, math, os, time \n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.optim.lr_scheduler import StepLR, ExponentialLR\n","\n","import numpy as np\n","np.set_printoptions(threshold=np.inf)\n","\n","\n","SEED = 1234\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"J_vW4pdITOJ8","executionInfo":{"status":"ok","timestamp":1669883714015,"user_tz":-420,"elapsed":12780,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Các hàm cần thiết để train mô hình"],"metadata":{"id":"heKBCoSQTwLD"}},{"cell_type":"code","source":["# Các siêu tham số của mô hình\n","INPUT_DIM = 6\n","OUTPUT_DIM = 1\n","ENC_HID_DIM = 50\n","DEC_HID_DIM = 50\n","ENC_DROPOUT = 0.1\n","DEC_DROPOUT = 0.1\n","ECN_Layers = 1\n","DEC_Layers = 1\n","LR = 0.001  # learning rate\n","CLIP = 1\n","EPOCHS = 50 ########\n","BATCH_SIZE = 1\n","N_output = 6"],"metadata":{"id":"cmX1b-_aT4hs","executionInfo":{"status":"ok","timestamp":1669883714015,"user_tz":-420,"elapsed":13,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def train(model, optimizer, criterion, X_train_left, X_train_right, y_train):\n","\n","    iter_per_epoch = int(np.ceil(X_train_left.shape[0] * 1. / BATCH_SIZE))\n","    #Làm tròn lên\n","    iter_losses = np.zeros(EPOCHS * iter_per_epoch)\n","\n","    n_iter = 0\n","\n","    perm_idx = np.random.permutation(X_train_left.shape[0])\n","    # Hoán vị ngẫu nhiên một dãy hoặc trả về một dãy đã hoán vị. - permutation\n","\n","    # train for each batch\n","\n","    for t_i in range(0, X_train_left.shape[0], BATCH_SIZE): # (0, 6526, 1)\n","        batch_idx = perm_idx[t_i:(t_i + BATCH_SIZE)]\n","\n","        x_train_left_batch = np.take(X_train_left, batch_idx, axis=0)\n","        # Lấy ra 10 giá trị x_train_left ở vị trí batch_idx (1, 10, 6)\n","\n","        x_train_right_batch = np.take(X_train_right, batch_idx, axis=0)\n","        # Lấy ra 10 giá trị x_train_right ở vị trí batch_idx (1, 10, 6)\n","\n","        y_train_batch = np.take(y_train, batch_idx, axis=0)\n","        # Lấy ra 6 giá trị y_train cần điền khuyết ở vị trí batch_index (1,6,1)\n","\n","        loss = train_iteration(model, optimizer, criterion, CLIP,x_train_left_batch, x_train_right_batch, y_train_batch)\n","\n","        iter_losses[t_i // BATCH_SIZE] = loss\n","\n","        n_iter += 1\n","\n","    return np.mean(iter_losses[range(0, iter_per_epoch)])\n","\n","\n","def train_iteration(model, optimizer, criterion, clip, X_train_left, X_train_right, y_train):\n","\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    X_train_left = np.transpose(X_train_left, [1, 0, 2])  # (10, 6526, 6)\n","    X_train_right = np.transpose(X_train_right, [1, 0, 2]) # (10, 6526, 6)\n","    #X_train_left22[9].shape (6526, 6)\n","\n","    y_train = np.transpose(y_train, [1, 0, 2])  #(6, 6526, 1)\n","\n","    X_train_left_tensor = numpy_to_tvar(X_train_left)  \n","    X_train_right_tensor = numpy_to_tvar(X_train_right)\n","    y_train_tensor = numpy_to_tvar(y_train)   # torch.Size([6, 6526, 1])\n","\n","    output, atten = model(X_train_left_tensor, X_train_right_tensor, y_train_tensor)\n","\n","    output = output.permute(1, 0, 2)\n","    y_train_tensor = y_train_tensor.permute(1, 0, 2)\n","\n","    loss_mse, loss_shape, loss_temporal = torch.tensor(0), torch.tensor(0), torch.tensor(0)\n","\n","    loss, loss_shape, loss_temporal = dilate_loss(y_train_tensor, output, 0.85, 0.01, device)\n","\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","    optimizer.step()\n","    # # for AdamW+Cyclical Learning Rate\n","\n","    return loss.item()\n","\n","\n","\n","########## Đánh giá  ################\n","def evaluate(model, criterion, X_test_left, X_test_right, y_test):\n","\n","  epoch_loss = 0\n","  iter_per_epoch = int(np.ceil(X_test_left.shape[0] * 1. / BATCH_SIZE))\n","  iter_losses = np.zeros(EPOCHS * iter_per_epoch)\n","  iter_multiloss = [np.zeros(EPOCHS * iter_per_epoch), np.zeros(EPOCHS * iter_per_epoch),\n","                    np.zeros(EPOCHS * iter_per_epoch), np.zeros(EPOCHS * iter_per_epoch)]\n","  perm_idx = np.random.permutation(X_test_left.shape[0])\n","\n","  n_iter = 0\n","\n","  with torch.no_grad():\n","    for t_i in range(0, X_test_left.shape[0], BATCH_SIZE):\n","      batch_idx = perm_idx[t_i:(t_i + BATCH_SIZE)]\n","\n","      x_test_left_batch = np.take(X_test_left, batch_idx, axis=0)\n","      x_test_right_batch = np.take(X_test_right, batch_idx, axis=0)\n","      y_test_batch = np.take(y_test, batch_idx, axis=0)\n","\n","      loss, mae, rmsle, rmse, loss_tdi = evaluate_iteration(model, criterion, x_test_left_batch, x_test_right_batch, y_test_batch)\n","      iter_losses[t_i // BATCH_SIZE] = loss\n","      iter_multiloss[0][t_i // BATCH_SIZE] = mae\n","      iter_multiloss[1][t_i // BATCH_SIZE] = rmsle\n","      iter_multiloss[2][t_i // BATCH_SIZE] = rmse\n","      iter_multiloss[3][t_i // BATCH_SIZE] = loss_tdi\n","\n","      n_iter += 1\n","\n","  return np.mean(iter_losses[range(0, iter_per_epoch)]), np.mean(iter_multiloss[0][range(0, iter_per_epoch)]), np.mean(\n","      iter_multiloss[1][range(0, iter_per_epoch)]), np.mean(iter_multiloss[2][range(0, iter_per_epoch)]), np.mean(iter_multiloss[3][range(0, iter_per_epoch)])\n","\n","\n","def evaluate_iteration(model, criterion, X_test_left, X_test_right, y_test):\n","  model.eval()\n","\n","  x_test_left = np.transpose(X_test_left, [1, 0, 2])\n","  x_test_right = np.transpose(X_test_right, [1, 0, 2])\n","  y_test = np.transpose(y_test, [1, 0, 2])\n","\n","  x_test_left_tensor = numpy_to_tvar(x_test_left)\n","  x_test_right_tensor = numpy_to_tvar(x_test_right)\n","\n","  y_test_tensor = numpy_to_tvar(y_test)\n","\n","  output, atten = model(x_test_left_tensor,x_test_right_tensor, y_test_tensor, 0)\n","\n","  loss = criterion(output, y_test_tensor)\n","  loss_mse, loss_dtw, loss_tdi = 0, 0, 0\n","  loss_mae, loss_RMSLE, loss_RMSE = 0, 0, 0\n","\n","  for k in range(BATCH_SIZE):\n","    target_k_cpu = y_test_tensor[:, k, 0:1].view(-1).detach().cpu().numpy()\n","    output_k_cpu = output[:, k, 0:1].view(-1).detach().cpu().numpy()\n","\n","    loss_dtw += dtw(target_k_cpu, output_k_cpu)\n","    path, sim = dtw_path(target_k_cpu, output_k_cpu)\n","\n","    Dist = 0\n","    for i, j in path:\n","        Dist += (i-j)*(i-j)\n","    loss_tdi += Dist / (N_output*N_output)\n","\n","    loss_mae += mean_absolute_error(target_k_cpu, output_k_cpu)\n","    loss_RMSLE += np.sqrt(mean_squared_error(target_k_cpu, output_k_cpu))\n","    loss_RMSE += np.sqrt(mean_squared_error(target_k_cpu, output_k_cpu))\n","\n","  loss_dtw = loss_dtw / BATCH_SIZE\n","  loss_tdi = loss_tdi / BATCH_SIZE\n","  loss_mae = loss_mae / BATCH_SIZE\n","  loss_RMSLE = loss_RMSLE / BATCH_SIZE\n","  loss_RMSE = loss_RMSE / BATCH_SIZE\n","\n","\n","  return loss.item(), loss_mae, loss_RMSLE, loss_RMSE, loss_dtw\n","\n","\n","\n","########## Pridict ###########\n","def predict_ts(model, X_test_left, X_test_right, scaler_y, max_gap_size=6, BATCH_SIZE=1, device=device):\n","  model.eval()\n","\n","  with torch.no_grad():\n","\n","    x_test_left = np.transpose(X_test_left, [1, 0, 2])\n","    x_test_right = np.transpose(X_test_right, [1, 0, 2])\n","\n","    empty_y_tensor = torch.zeros(max_gap_size, BATCH_SIZE,1).to(device)\n","\n","    x_test_left_tensor = numpy_to_tvar(x_test_left)\n","    x_test_right_tensor = numpy_to_tvar(x_test_right)\n","\n","    output, _ = model(x_test_left_tensor,x_test_right_tensor, empty_y_tensor, 0)\n","\n","    output = torch.squeeze(output)\n","    output = torch.transpose(output, 0, 1)\n","    output = torch.flatten(output)\n","\n","    # scalar\n","    output_numpy = output.cpu().data.numpy()\n","    output_numpy_origin = scaler_y.inverse_transform(output_numpy.reshape(-1, 1))\n","\n","  return output_numpy_origin, output_numpy\n"],"metadata":{"id":"269rfQObTxeR","executionInfo":{"status":"ok","timestamp":1669883714016,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["### Chia train/test set\n","\n","len_before = 10\n","len_after = 10\n","graphs = 20\n","\n","## Train/test set cho NO3\n","(x_train, y_train), (x_test, y_test), (scaler_x,scaler_y) = test_qld_single_station(len_before, len_after, graphs)\n","\n","\n","## Train/test set cho Level\n","#(x_train, y_train), (x_test, y_test), (scaler_x,scaler_y) = test_qld_single_station_Level(len_before, len_after, graphs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKh_w1D0Ubue","executionInfo":{"status":"ok","timestamp":1669883776763,"user_tz":-420,"elapsed":1494,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}},"outputId":"1e27a8e7-9fd7-44b0-a504-c62e4ad20815"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["train_preprocess:(6551, 6)\n","test_preprocess:(2207, 6)\n","concatenated_x (6512, 40, 6)\n","concatenated_y (6512, 20)\n","len_all_case (6512,)\n","len_before_all_case (6512,)\n","x:(6512, 40, 6)\n","y:(6512, 20, 1)\n","concatenated_x (2168, 40, 6)\n","concatenated_y (2168, 20)\n","len_all_case (2168,)\n","len_before_all_case (2168,)\n","x:(2168, 40, 6)\n","y:(2168, 20, 1)\n","x_train:(6512, 40, 6)\n","y_train:(6512, 20, 1)\n","x_test:(2168, 40, 6)\n","y_test:(2168, 20, 1)\n","split train/test array\n","(2168, 10, 6)\n","(2168, 20, 6)\n","(2168, 10, 6)\n"]}]},{"cell_type":"code","source":["# Dòng 250 - 251\n","print('split train/test array')\n","x_test_list = np.split(x_test, [len_before, len_before+graphs], axis=1)\n","x_train_list = np.split(x_train, [len_before, len_before+graphs], axis=1)\n","\n","for i in x_test_list:\n","    print(i.shape)\n","print('==========')\n","for i in x_train_list:\n","    print(i.shape)\n","\n","X_train_left = x_train_list[0]\n","X_train_right = x_train_list[2]\n","X_test_left = x_test_list[0]\n","X_test_right = x_test_list[2]\n","\n","print('X_train_left:{}'.format(X_train_left.shape))\n","print('X_train_right:{}'.format(X_train_right.shape))\n","print('X_test_left:{}'.format(X_test_left.shape))\n","print('X_test_right:{}'.format(X_test_right.shape))\n","\n","# fit batchsize, kiểm tra kích thước sau khi xóa khuyết\n","X_train_left = X_train_left[:]\n","X_train_right = X_train_right[:]\n","print(X_train_left.shape, X_train_right.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Th3F-OWJU5iB","executionInfo":{"status":"ok","timestamp":1669883778555,"user_tz":-420,"elapsed":3,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}},"outputId":"13e52124-aec3-4a01-9dea-c6ae2010b9cb"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["split train/test array\n","(2168, 10, 6)\n","(2168, 20, 6)\n","(2168, 10, 6)\n","==========\n","(6512, 10, 6)\n","(6512, 20, 6)\n","(6512, 10, 6)\n","X_train_left:(6512, 10, 6)\n","X_train_right:(6512, 10, 6)\n","X_test_left:(2168, 10, 6)\n","X_test_right:(2168, 10, 6)\n","(6512, 10, 6) (6512, 10, 6)\n"]}]},{"cell_type":"code","source":["'''pp='/content/stop_early_Level20.pt'\n","model.load_state_dict(torch.load(pp, map_location=torch.device('cpu')))\n","\n","test_loss, test_mae, test_rmsle, test_rmse, test_tdi = evaluate(model, criterion, X_test_left, X_test_right, y_test)\n","\n","print(f'| Test Loss: {test_loss:.4f} | Test PPL: {math.exp(test_loss):7.4f} |')\n","print(f'| MAE: {test_mae:.4f} | Test PPL: {math.exp(test_mae):7.4f} |')\n","print(f'| RMSLE: {test_rmsle:.4f} | Test PPL: {math.exp(test_rmsle):7.4f} |')\n","print(f'| RMSE: {test_rmse:.4f} | Test PPL: {math.exp(test_rmse):7.4f} |')\n","print(f'| DTW: {test_tdi:.4f} | Test PPL: {math.exp(test_tdi):7.4f} |')'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"Cf6L749T5O66","executionInfo":{"status":"ok","timestamp":1669825156945,"user_tz":-420,"elapsed":25,"user":{"displayName":"Nghĩa Nguyễn Hiếu","userId":"12372069855683604213"}},"outputId":"86ede3ba-0f10-47fe-9b87-580eee8575e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"pp='/content/stop_early_Level20.pt'\\nmodel.load_state_dict(torch.load(pp, map_location=torch.device('cpu')))\\n\\ntest_loss, test_mae, test_rmsle, test_rmse, test_tdi = evaluate(model, criterion, X_test_left, X_test_right, y_test)\\n\\nprint(f'| Test Loss: {test_loss:.4f} | Test PPL: {math.exp(test_loss):7.4f} |')\\nprint(f'| MAE: {test_mae:.4f} | Test PPL: {math.exp(test_mae):7.4f} |')\\nprint(f'| RMSLE: {test_rmsle:.4f} | Test PPL: {math.exp(test_rmsle):7.4f} |')\\nprint(f'| RMSE: {test_rmse:.4f} | Test PPL: {math.exp(test_rmse):7.4f} |')\\nprint(f'| DTW: {test_tdi:.4f} | Test PPL: {math.exp(test_tdi):7.4f} |')\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### Model"],"metadata":{"id":"8mRWR9wHVMhs"}},{"cell_type":"code","source":["# Model\n","cross_attn = Cross_Attention(ENC_HID_DIM, DEC_HID_DIM)\n","enc = Shared_Encoder(INPUT_DIM, ENC_HID_DIM, DEC_HID_DIM, ECN_Layers, DEC_Layers, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_Layers, DEC_DROPOUT, cross_attn)\n","\n","model = DualSSIM(enc, dec, device).to(device)\n","model.apply(init_weights)\n","\n","print(model)\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-48KlGPVNxt","executionInfo":{"status":"ok","timestamp":1669883782354,"user_tz":-420,"elapsed":339,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}},"outputId":"deb0df03-4ba1-44f4-8c5f-446271f13837"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["DualSSIM(\n","  (shared_encoder): Shared_Encoder(\n","    (input_linear): Linear(in_features=6, out_features=50, bias=True)\n","    (gru_left): GRU(50, 50, bidirectional=True)\n","    (gru_right): GRU(50, 50, bidirectional=True)\n","    (output_linear_left): Linear(in_features=100, out_features=50, bias=True)\n","    (output_linear_right): Linear(in_features=100, out_features=50, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Cross_Attention(\n","      (attn): Linear(in_features=150, out_features=50, bias=True)\n","    )\n","    (input_dec): Linear(in_features=1, out_features=50, bias=True)\n","    (gru): GRU(150, 50)\n","    (out): Linear(in_features=200, out_features=1, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","The model has 109,851 trainable parameters\n"]}]},{"cell_type":"markdown","source":["### Tối ưu hóa"],"metadata":{"id":"dEux5VVSVSjU"}},{"cell_type":"code","source":["#Adam\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999))\n","\n","# warmup\n","scheduler_steplr = StepLR(optimizer, step_size=10, gamma=0.1)\n","scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=5)\n","\n","criterion = nn.MSELoss()\n","# Dừng sớm\n","# Khởi tạo đối tượng dừng sớm\n","patience = 10\n","\n","## Stop early cho NO3\n","#early_stopping = EarlyStopping(output_path='/content/stop_early.pt',patience=patience,verbose=True)\n","\n","\n","#Stop early cho Level\n","early_stopping = EarlyStopping(output_path='/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/Results_Kết quả chạy model_NO3/no3_20/stop_early_NO3_20.pt',patience=patience,verbose=True)\n","\n","optimizer.zero_grad()\n","optimizer.step()"],"metadata":{"id":"w9iCrsVjVWzQ","executionInfo":{"status":"ok","timestamp":1669883785088,"user_tz":-420,"elapsed":2,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["optimizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rlWkvOJVau4","executionInfo":{"status":"ok","timestamp":1669825156950,"user_tz":-420,"elapsed":19,"user":{"displayName":"Nghĩa Nguyễn Hiếu","userId":"12372069855683604213"}},"outputId":"fcf52755-6fce-449b-8659-2696a78b48be"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Adam (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    eps: 1e-08\n","    foreach: None\n","    initial_lr: 0.001\n","    lr: 0.0\n","    maximize: False\n","    weight_decay: 0\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["### Trainning"],"metadata":{"id":"7yJYGmS-Vieq"}},{"cell_type":"code","source":["best_valid_loss = float('inf')\n","for epoch in range(EPOCHS):\n","\n","    scheduler_warmup.step(epoch)\n","    train_epoch_losses = np.zeros(EPOCHS)\n","    evaluate_epoch_losses = np.zeros(EPOCHS)\n","\n","\n","    start_time = time.time()\n","    train_loss = train(model, optimizer, criterion, X_train_left, X_train_right, y_train)\n","    valid_loss,test_mae, test_rmsle, test_rmse, test_tdi = evaluate(model, criterion, X_test_left, X_test_right, y_test)\n","    end_time = time.time()\n","\n","\n","    train_epoch_losses[epoch] = train_loss\n","    evaluate_epoch_losses[epoch] = valid_loss\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    # Dừng sớm cần đánh giá hàm loss để xem đã giảm hay chưa, và nếu có thì sẽ dừng ở mô hình hiện tại\n","    early_stopping(valid_loss, model)\n","\n","    if early_stopping.early_stop:\n","         print(\"Early stopping\")\n","         break\n","    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print( f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print( f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n","    print(f'| MAE: {test_mae:.4f} | Test PPL: {math.exp(test_mae):7.4f} |')\n","    print(f'| RMSLE: {test_rmsle:.4f} | Test PPL: {math.exp(test_rmsle):7.4f} |')\n","    print(f'| RMSE: {test_rmse:.4f} | Test PPL: {math.exp(test_rmse):7.4f} |')\n","    print(f'| TDI: {test_tdi:.4f} | Test PPL: {math.exp(test_tdi):7.4f} |')"],"metadata":{"id":"5wsgINrbVj24","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669830353750,"user_tz":-420,"elapsed":5196815,"user":{"displayName":"Nghĩa Nguyễn Hiếu","userId":"12372069855683604213"}},"outputId":"28a0e97b-4d1e-44d1-abb4-4c1d2d0c410f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss decreased (inf --> 0.148679).  Saving model ...\n","Epoch: 01 | Time: 5m 38s\n","\tTrain Loss: 0.217 | Train PPL:   1.242\n","\t Val. Loss: 0.149 |  Val. PPL:   1.160\n","| MAE: 0.2544 | Test PPL:  1.2897 |\n","| RMSLE: 0.2971 | Test PPL:  1.3460 |\n","| RMSE: 0.2971 | Test PPL:  1.3460 |\n","| TDI: 1.3287 | Test PPL:  3.7762 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss decreased (0.148679 --> 0.094697).  Saving model ...\n","Epoch: 02 | Time: 5m 23s\n","\tTrain Loss: -0.148 | Train PPL:   0.863\n","\t Val. Loss: 0.095 |  Val. PPL:   1.099\n","| MAE: 0.2012 | Test PPL:  1.2229 |\n","| RMSLE: 0.2417 | Test PPL:  1.2734 |\n","| RMSE: 0.2417 | Test PPL:  1.2734 |\n","| TDI: 0.9633 | Test PPL:  2.6203 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss decreased (0.094697 --> 0.076612).  Saving model ...\n","Epoch: 03 | Time: 5m 26s\n","\tTrain Loss: -0.165 | Train PPL:   0.848\n","\t Val. Loss: 0.077 |  Val. PPL:   1.080\n","| MAE: 0.1805 | Test PPL:  1.1978 |\n","| RMSLE: 0.2188 | Test PPL:  1.2446 |\n","| RMSE: 0.2188 | Test PPL:  1.2446 |\n","| TDI: 0.8559 | Test PPL:  2.3536 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 1 out of 10\n","Epoch: 04 | Time: 5m 23s\n","\tTrain Loss: -0.168 | Train PPL:   0.845\n","\t Val. Loss: 0.080 |  Val. PPL:   1.084\n","| MAE: 0.1770 | Test PPL:  1.1937 |\n","| RMSLE: 0.2174 | Test PPL:  1.2428 |\n","| RMSE: 0.2174 | Test PPL:  1.2428 |\n","| TDI: 0.7982 | Test PPL:  2.2216 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 2 out of 10\n","Epoch: 05 | Time: 5m 22s\n","\tTrain Loss: -0.168 | Train PPL:   0.845\n","\t Val. Loss: 0.083 |  Val. PPL:   1.087\n","| MAE: 0.1879 | Test PPL:  1.2067 |\n","| RMSLE: 0.2265 | Test PPL:  1.2542 |\n","| RMSE: 0.2265 | Test PPL:  1.2542 |\n","| TDI: 0.8743 | Test PPL:  2.3971 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss decreased (0.076612 --> 0.052211).  Saving model ...\n","Epoch: 06 | Time: 5m 22s\n","\tTrain Loss: -0.169 | Train PPL:   0.845\n","\t Val. Loss: 0.052 |  Val. PPL:   1.054\n","| MAE: 0.1532 | Test PPL:  1.1656 |\n","| RMSLE: 0.1854 | Test PPL:  1.2037 |\n","| RMSE: 0.1854 | Test PPL:  1.2037 |\n","| TDI: 0.7019 | Test PPL:  2.0175 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 1 out of 10\n","Epoch: 07 | Time: 5m 23s\n","\tTrain Loss: -0.170 | Train PPL:   0.844\n","\t Val. Loss: 0.070 |  Val. PPL:   1.073\n","| MAE: 0.1729 | Test PPL:  1.1888 |\n","| RMSLE: 0.2103 | Test PPL:  1.2341 |\n","| RMSE: 0.2103 | Test PPL:  1.2341 |\n","| TDI: 0.7184 | Test PPL:  2.0512 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 2 out of 10\n","Epoch: 08 | Time: 5m 24s\n","\tTrain Loss: -0.170 | Train PPL:   0.843\n","\t Val. Loss: 0.062 |  Val. PPL:   1.064\n","| MAE: 0.1508 | Test PPL:  1.1628 |\n","| RMSLE: 0.1896 | Test PPL:  1.2087 |\n","| RMSE: 0.1896 | Test PPL:  1.2087 |\n","| TDI: 0.6560 | Test PPL:  1.9271 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 3 out of 10\n","Epoch: 09 | Time: 5m 24s\n","\tTrain Loss: -0.173 | Train PPL:   0.841\n","\t Val. Loss: 0.069 |  Val. PPL:   1.071\n","| MAE: 0.1629 | Test PPL:  1.1769 |\n","| RMSLE: 0.2024 | Test PPL:  1.2243 |\n","| RMSE: 0.2024 | Test PPL:  1.2243 |\n","| TDI: 0.7770 | Test PPL:  2.1750 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 4 out of 10\n","Epoch: 10 | Time: 5m 23s\n","\tTrain Loss: -0.174 | Train PPL:   0.840\n","\t Val. Loss: 0.113 |  Val. PPL:   1.120\n","| MAE: 0.2119 | Test PPL:  1.2360 |\n","| RMSLE: 0.2557 | Test PPL:  1.2913 |\n","| RMSE: 0.2557 | Test PPL:  1.2913 |\n","| TDI: 0.8443 | Test PPL:  2.3263 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 5 out of 10\n","Epoch: 11 | Time: 5m 23s\n","\tTrain Loss: -0.172 | Train PPL:   0.842\n","\t Val. Loss: 0.054 |  Val. PPL:   1.056\n","| MAE: 0.1434 | Test PPL:  1.1542 |\n","| RMSLE: 0.1776 | Test PPL:  1.1943 |\n","| RMSE: 0.1776 | Test PPL:  1.1943 |\n","| TDI: 0.6785 | Test PPL:  1.9710 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 6 out of 10\n","Epoch: 12 | Time: 5m 23s\n","\tTrain Loss: -0.175 | Train PPL:   0.839\n","\t Val. Loss: 0.057 |  Val. PPL:   1.059\n","| MAE: 0.1474 | Test PPL:  1.1589 |\n","| RMSLE: 0.1805 | Test PPL:  1.1979 |\n","| RMSE: 0.1805 | Test PPL:  1.1979 |\n","| TDI: 0.6874 | Test PPL:  1.9885 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 7 out of 10\n","Epoch: 13 | Time: 5m 25s\n","\tTrain Loss: -0.174 | Train PPL:   0.840\n","\t Val. Loss: 0.066 |  Val. PPL:   1.069\n","| MAE: 0.1567 | Test PPL:  1.1697 |\n","| RMSLE: 0.1958 | Test PPL:  1.2163 |\n","| RMSE: 0.1958 | Test PPL:  1.2163 |\n","| TDI: 0.6694 | Test PPL:  1.9531 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 8 out of 10\n","Epoch: 14 | Time: 5m 23s\n","\tTrain Loss: -0.176 | Train PPL:   0.838\n","\t Val. Loss: 0.065 |  Val. PPL:   1.067\n","| MAE: 0.1539 | Test PPL:  1.1663 |\n","| RMSLE: 0.1934 | Test PPL:  1.2133 |\n","| RMSE: 0.1934 | Test PPL:  1.2133 |\n","| TDI: 0.7176 | Test PPL:  2.0495 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 9 out of 10\n","Epoch: 15 | Time: 5m 23s\n","\tTrain Loss: -0.176 | Train PPL:   0.839\n","\t Val. Loss: 0.075 |  Val. PPL:   1.078\n","| MAE: 0.1633 | Test PPL:  1.1774 |\n","| RMSLE: 0.2022 | Test PPL:  1.2240 |\n","| RMSE: 0.2022 | Test PPL:  1.2240 |\n","| TDI: 0.7139 | Test PPL:  2.0419 |\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/utils/dilate_loss.py:242: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  Omega =  pairwise_distances(torch.range(1,N_output).view(N_output,1)).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["EarlyStopping counter: 10 out of 10\n","Early stopping\n"]}]},{"cell_type":"markdown","source":["### Đánh giá"],"metadata":{"id":"s9scFeCVVxyk"}},{"cell_type":"code","source":["#Load NO3\n","#model.load_state_dict(torch.load('/content/stop_early.pt'))\n","\n","#Load Level\n","pp='/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/Results_Kết quả chạy model_NO3/no3_20/stop_early_NO3_20.pt'\n","\n","model.load_state_dict(torch.load(pp))\n","\n","test_loss, test_mae, test_rmsle, test_rmse, test_tdi = evaluate(model, criterion, X_test_left, X_test_right, y_test)\n","\n","print(f'| Test Loss: {test_loss:.4f} | Test PPL: {math.exp(test_loss):7.4f} |')\n","print(f'| MAE: {test_mae:.4f} | Test PPL: {math.exp(test_mae):7.4f} |')\n","print(f'| RMSLE: {test_rmsle:.4f} | Test PPL: {math.exp(test_rmsle):7.4f} |')\n","print(f'| RMSE: {test_rmse:.4f} | Test PPL: {math.exp(test_rmse):7.4f} |')\n","print(f'| DTW: {test_tdi:.4f} | Test PPL: {math.exp(test_tdi):7.4f} |')"],"metadata":{"id":"RaYNUx3bVzHo","executionInfo":{"status":"ok","timestamp":1669883836109,"user_tz":-420,"elapsed":25769,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"72633ccd-4c9a-45bb-fd64-1d657d6d7907"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["| Test Loss: 0.0522 | Test PPL:  1.0536 |\n","| MAE: 0.1532 | Test PPL:  1.1656 |\n","| RMSLE: 0.1854 | Test PPL:  1.2037 |\n","| RMSE: 0.1854 | Test PPL:  1.2037 |\n","| DTW: 0.7019 | Test PPL:  2.0175 |\n"]}]},{"cell_type":"markdown","source":["### Predict"],"metadata":{"id":"9ipcVtsTV4SA"}},{"cell_type":"code","source":["outputs_ori, outputs_scal = predict_ts(model, X_test_left, X_test_right, scaler_y, max_gap_size=6, BATCH_SIZE=1, device=device)\n","\n","print('==================')\n","print('outputs_ori:{}'.format(outputs_ori.shape))\n","print('==================')\n","print('outputs_scal:{}'.format(outputs_scal.shape))\n","\n","\n","# N03\n","#np.save('/content/{}_ori'.format('Nitrate6_1012'), outputs_ori)\n","#np.save('/content/{}_scal'.format('Nitrate6_1012'), outputs_scal)\n","\n","#/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/Results_Kết quả chạy model_NO3\n","#Level  \n","np.save('/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/Results_Kết quả chạy model_NO3/no3_20/{}_ori'.format('NO3_20_1012'), outputs_ori)\n","np.save('/content/gdrive/MyDrive/Project_HK1_2022/Data_Visualization/Code/Model_dienkhuyet/Results_Kết quả chạy model_NO3/no3_20/{}_scal'.format('NO3_20_1012'), outputs_scal)"],"metadata":{"id":"GEZDSAVTV6Ah","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669830379792,"user_tz":-420,"elapsed":1250,"user":{"displayName":"Nghĩa Nguyễn Hiếu","userId":"12372069855683604213"}},"outputId":"9389bada-3317-4027-df76-7f06b115a126"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================\n","outputs_ori:(13008, 1)\n","==================\n","outputs_scal:(13008,)\n"]}]}]}